{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import floweaver\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GFOP sample type metadata "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def get_sample_types(gfop_metadata, simple_complex=None):\n",
    "    if simple_complex is not None:\n",
    "        gfop_metadata = gfop_metadata[\n",
    "            gfop_metadata['simple_complex'] == simple_complex]\n",
    "    col_sample_types = [f'sample_type_group{i}' for i in range(1, 7)]\n",
    "    return (gfop_metadata[['filename', *col_sample_types]]\n",
    "            .set_index('filename'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gfop_metadata = pd.read_csv(\n",
    "    '../data/11442_foodomics_multiproject_metadata.txt', sep='\\t')\n",
    "# First row is empty.\n",
    "gfop_metadata = gfop_metadata.drop(index=0)\n",
    "# Remove trailing whitespace.\n",
    "gfop_metadata = gfop_metadata.apply(lambda col: col.str.strip()\n",
    "                                    if col.dtype == 'object' else col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Food type at different metadata levels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _get_flows(gnps_network, sample_types, groups_included,\n",
    "               max_hierarchy_level):\n",
    "    # Select GNPS job groups.\n",
    "    groups = {f'G{i}' for i in range(1, 7)}\n",
    "    groups_excluded = groups - set(groups_included)\n",
    "    df_selected = gnps_network[\n",
    "        (gnps_network[groups_included] > 0).all(axis=1) &\n",
    "        (gnps_network[groups_excluded] == 0).all(axis=1)].copy()\n",
    "    filenames = (df_selected['UniqueFileSources'].str.split('|')\n",
    "                 .explode())#.unique())\n",
    "    # Select food hierarchy levels.\n",
    "    sample_types = sample_types[[\n",
    "        f'sample_type_group{i}' for i in range(1, max_hierarchy_level + 1)]]\n",
    "    # Match the GNPS job results to the food sample types.\n",
    "    sample_types_selected = sample_types.reindex(filenames)\n",
    "    sample_types_selected = sample_types_selected.dropna()\n",
    "    # Discard samples that occur less frequent than water (blank).\n",
    "    water_count = ((sample_types_selected['sample_type_group1'] == 'water')\n",
    "                   .sum())\n",
    "    sample_counts = sample_types_selected[\n",
    "        f'sample_type_group{max_hierarchy_level}'].value_counts()\n",
    "    sample_counts_valid = sample_counts.index[sample_counts > water_count]\n",
    "    sample_types_selected = sample_types_selected[sample_types_selected[\n",
    "        f'sample_type_group{max_hierarchy_level}'].isin(sample_counts_valid)]\n",
    "    # Get the flows between consecutive food hierarchy levels.\n",
    "    flows, processes = [], []\n",
    "    for i in range(1, max_hierarchy_level):\n",
    "        g1, g2 = f'sample_type_group{i}', f'sample_type_group{i + 1}'\n",
    "        flow = (sample_types_selected.groupby([g1, g2]).size()\n",
    "                .reset_index().rename(columns={g1: 'source', g2: 'target',\n",
    "                                               0: 'value'}))\n",
    "        flow['source'] = flow['source'] + f'_{i}'\n",
    "        flow['target'] = flow['target'] + f'_{i + 1}'\n",
    "        flow['type'] = flow['target']\n",
    "        flows.append(flow)\n",
    "        process = pd.concat([flow['source'], flow['target']],\n",
    "                            ignore_index=True).to_frame()\n",
    "        process['level'] = [*np.repeat(i, len(flow['source'])),\n",
    "                            *np.repeat(i + 1, len(flow['target']))]\n",
    "        processes.append(process)\n",
    "    return (pd.concat(flows, ignore_index=True),\n",
    "            pd.concat(processes, ignore_index=True).drop_duplicates()\n",
    "            .rename(columns={0: 'id'}).set_index('id'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def plot_flows(gnps_network, sample_types, groups_included,\n",
    "               sample_type_hierarchy, max_hierarchy_level=4, filename=None):\n",
    "    flows, processes = _get_flows(\n",
    "        gnps_network, sample_types, groups_included, max_hierarchy_level)\n",
    "    dataset = floweaver.Dataset(flows, dim_process=processes)\n",
    "\n",
    "    labels = (sample_type_hierarchy\n",
    "              .reindex(set(flows['source']) | set(flows['target']))\n",
    "              .sort_values('order_num').index)\n",
    "    nodes, ordering, bundles = {}, [], []\n",
    "    for level in processes['level'].unique():\n",
    "        nodes[f'level {level}'] = floweaver.ProcessGroup(f'level == {level}')\n",
    "        nodes[f'level {level}'].partition = floweaver.Partition.Simple(\n",
    "            'process', labels[labels.str.endswith(f'_{level}')][::-1])\n",
    "\n",
    "        ordering.append([f'level {level}'])\n",
    "\n",
    "        if level + 1 in processes['level'].unique():\n",
    "            bundles.append(floweaver.Bundle(f'level {level}',\n",
    "                                            f'level {level + 1}'))\n",
    "\n",
    "    sdd = floweaver.SankeyDefinition(\n",
    "        nodes, bundles, ordering, flow_partition=dataset.partition('type'))\n",
    "    palette = sample_type_hierarchy['color_code'].dropna().to_dict()\n",
    "    return floweaver.weave(sdd, dataset, palette=palette)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_types_simple = get_sample_types(gfop_metadata, 'simple')\n",
    "sample_types_complex = get_sample_types(gfop_metadata, 'complex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_type_hierarchy = (\n",
    "    pd.read_csv('../data/sample_type_hierarchy.csv')\n",
    "    .set_index('descriptor').sort_values('order_num'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gnps_network = pd.read_csv(\n",
    "    '../data/METABOLOMICS-SNETS-V2-9a90bd12-view_all_clusters_withID_beta-main.tsv',\n",
    "    sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'onr'\n",
    "simple_complex = 'simple'\n",
    "groups = '2', '4'\n",
    "max_level = 5\n",
    "\n",
    "if simple_complex == 'simple':\n",
    "    sample_types = sample_types_simple\n",
    "elif simple_complex == 'complex':\n",
    "    sample_types = sample_types_complex\n",
    "else:\n",
    "    raise ValueError('Unknown sample type')\n",
    "sankey_data = plot_flows(\n",
    "    gnps_network, sample_types, [f'G{g}' for g in groups],\n",
    "    sample_type_hierarchy, max_level)\n",
    "width = max_level * 150 + 300\n",
    "height = math.ceil(\n",
    "    sum([1 for node in sankey_data.nodes\n",
    "         if node.title.endswith(str(max_level))]) * 50 / 100) * 100\n",
    "(sankey_data.to_widget(width=width, height=height, margins={\n",
    "    'left': 150, 'right': 150, 'top': -50, 'bottom': -50})\n",
    " .auto_save_png(f'flow_{dataset}_g{\"\".join(groups)}_level{max_level}_'\n",
    "                f'{simple_complex}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](fig1d_flow_onr_g24_level5_simple.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
